{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratory 23: More on Classification, Logistic Regression, and Discrete GOF Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "http://54.243.252.9:8000/hub/user-redirect/lab/tree/CECE-1330-PsuedoCourse/1-Lessons/Lesson23/Lab23/Lab23_Class.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full name: \n",
    "## R#: \n",
    "## HEX: \n",
    "## Title of the notebook\n",
    "## Date: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://expertsystem.com/wp-content/uploads/2017/03/machine-learning-definition-1024x268.jpeg) <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last session we talked about classification and logistic regression ... <br>\n",
    "\n",
    "![](https://memegenerator.net/img/instances/67603083.jpg) <br>\n",
    "\n",
    "### We discussed ...\n",
    "- __The difference between classification and regression problems__<br>\n",
    "- __The theory of logistic regression__  <br>    \n",
    "- __How to implement logistic regression in Python__ <br>\n",
    "- __Training and testing of models__\n",
    "- __And how to assess the performance of a classifier using confusion matrix and GOF metrics calculated based on it__\n",
    "    \n",
    "![](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcQOXYmL90i7rewXOQZBDvrgxFIsywM1_6HuBQ&usqp=CAU)\n",
    "\n",
    "### To avoid a situation like this, we will work on another example of logisitc regression today and use it to re-cover some of the fundamental concepts! \n",
    "\n",
    "<hr>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Credit Card Fraud Detection <br>\n",
    "\n",
    "![](https://i.pinimg.com/originals/5e/2e/a9/5e2ea94eb6d47c16ece524873234d199.png) <br>\n",
    "\n",
    "\n",
    "\n",
    "### For many companies, losses involving transaction fraud amount to more than 10% of their total expenses. The concern with these massive losses leads companies to constantly seek new solutions to prevent, detect and eliminate fraud. Machine Learning is one of the most promising technological weapons to combat financial fraud. The objective of this project is to create a simple Logistic Regression model capable of detecting fraud in credit card operations, thus seeking to minimize the risk and loss of the business.\n",
    "\n",
    "### The dataset used contains transactions carried out by European credit card holders that took place over two days in September 2013, and is a shorter version of a dataset that is available on kaggle at https://www.kaggle.com/mlg-ulb/creditcardfraud/version/3.\n",
    "\n",
    "### \"It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, … V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\"\n",
    "\n",
    "\n",
    "|Columns|Info.|\n",
    "|---:|---:|\n",
    "|Time |Number of seconds elapsed between this transaction and the first transaction in the dataset|\n",
    "|V1-V28 |Result of a PCA Dimensionality reduction to protect user identities and sensitive features(v1-v28)|\n",
    "|Amount |Transaction amount|\n",
    "|Class |1 for fraudulent transactions, 0 otherwise|\n",
    "\n",
    "\n",
    "*NOTE: Principal Component Analysis, or PCA, is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set.*\n",
    "\n",
    "<hr>\n",
    "\n",
    "*__Acknowledgements__*\n",
    "The dataset has been collected and analysed during a research collaboration of Worldline and the Machine Learning Group (http://mlg.ulb.ac.be) of ULB (Université Libre de Bruxelles) on big data mining and fraud detection.\n",
    "More details on current and past projects on related topics are available on https://www.researchgate.net/project/Fraud-detection-5 and the page of the DefeatFraud project\n",
    "\n",
    "Please cite the following works:\n",
    "\n",
    "*Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi. Calibrating Probability with Undersampling for Unbalanced Classification. In Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015*\n",
    "\n",
    "*Dal Pozzolo, Andrea; Caelen, Olivier; Le Borgne, Yann-Ael; Waterschoot, Serge; Bontempi, Gianluca. Learned lessons in credit card fraud detection from a practitioner perspective, Expert systems with applications,41,10,4915-4928,2014, Pergamon*\n",
    "\n",
    "*Dal Pozzolo, Andrea; Boracchi, Giacomo; Caelen, Olivier; Alippi, Cesare; Bontempi, Gianluca. Credit card fraud detection: a realistic modeling and a novel learning strategy, IEEE transactions on neural networks and learning systems,29,8,3784-3797,2018,IEEE*\n",
    "\n",
    "*Dal Pozzolo, Andrea Adaptive Machine learning for credit card fraud detection ULB MLG PhD thesis (supervised by G. Bontempi)*\n",
    "\n",
    "*Carcillo, Fabrizio; Dal Pozzolo, Andrea; Le Borgne, Yann-Aël; Caelen, Olivier; Mazzer, Yannis; Bontempi, Gianluca. Scarff: a scalable framework for streaming credit card fraud detection with Spark, Information fusion,41, 182-194,2018,Elsevier*\n",
    "\n",
    "*Carcillo, Fabrizio; Le Borgne, Yann-Aël; Caelen, Olivier; Bontempi, Gianluca. Streaming active learning strategies for real-life credit card fraud detection: assessment and visualization, International Journal of Data Science and Analytics, 5,4,285-300,2018,Springer International Publishing*\n",
    "\n",
    "*Bertrand Lebichot, Yann-Aël Le Borgne, Liyun He, Frederic Oblé, Gianluca Bontempi Deep-Learning Domain Adaptation Techniques for Credit Cards Fraud Detection, INNSBDDL 2019: Recent Advances in Big Data and Deep Learning, pp 78-88, 2019*\n",
    "\n",
    "*Fabrizio Carcillo, Yann-Aël Le Borgne, Olivier Caelen, Frederic Oblé, Gianluca Bontempi Combining Unsupervised and Supervised Learning in Credit Card Fraud Detection Information Sciences, 2019*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As you know by now, the first step is to load some necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn.metrics as metrics\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then, we should read the dataset and explore it using tools such as descriptive statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0     0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1     0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2     1 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3     1 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4     2 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the dataset:\n",
    "data = pd.read_csv(\"creditcard_m.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As expected, the dataset has 31 columns and the target variable is located in the last one. Let's check and see whether we have any missing values in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time      0\n",
       "V1        0\n",
       "V2        0\n",
       "V3        0\n",
       "V4        0\n",
       "V5        0\n",
       "V6        0\n",
       "V7        0\n",
       "V8        0\n",
       "V9        0\n",
       "V10       0\n",
       "V11       0\n",
       "V12       0\n",
       "V13       0\n",
       "V14       0\n",
       "V15       0\n",
       "V16       0\n",
       "V17       0\n",
       "V18       0\n",
       "V19       0\n",
       "V20       0\n",
       "V21       0\n",
       "V22       0\n",
       "V23       0\n",
       "V24       0\n",
       "V25       0\n",
       "V26       0\n",
       "V27       0\n",
       "V28       0\n",
       "Amount    0\n",
       "Class     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Great! No missing values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>140000.000000</td>\n",
       "      <td>140000.000000</td>\n",
       "      <td>140000.000000</td>\n",
       "      <td>140000.000000</td>\n",
       "      <td>140000.000000</td>\n",
       "      <td>140000.000000</td>\n",
       "      <td>140000.000000</td>\n",
       "      <td>140000.000000</td>\n",
       "      <td>140000.000000</td>\n",
       "      <td>140000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>140000.000000</td>\n",
       "      <td>140000.000000</td>\n",
       "      <td>140000.000000</td>\n",
       "      <td>140000.000000</td>\n",
       "      <td>140000.000000</td>\n",
       "      <td>140000.000000</td>\n",
       "      <td>140000.000000</td>\n",
       "      <td>140000.000000</td>\n",
       "      <td>140000.000000</td>\n",
       "      <td>140000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>51858.089636</td>\n",
       "      <td>-0.249409</td>\n",
       "      <td>0.017429</td>\n",
       "      <td>0.672713</td>\n",
       "      <td>0.139812</td>\n",
       "      <td>-0.282655</td>\n",
       "      <td>0.078898</td>\n",
       "      <td>-0.117062</td>\n",
       "      <td>0.065205</td>\n",
       "      <td>-0.092188</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.039503</td>\n",
       "      <td>-0.118547</td>\n",
       "      <td>-0.033419</td>\n",
       "      <td>0.012095</td>\n",
       "      <td>0.130218</td>\n",
       "      <td>0.023580</td>\n",
       "      <td>0.000651</td>\n",
       "      <td>0.002244</td>\n",
       "      <td>91.210270</td>\n",
       "      <td>0.001886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>20867.521978</td>\n",
       "      <td>1.816521</td>\n",
       "      <td>1.614340</td>\n",
       "      <td>1.268657</td>\n",
       "      <td>1.322410</td>\n",
       "      <td>1.307926</td>\n",
       "      <td>1.284004</td>\n",
       "      <td>1.166853</td>\n",
       "      <td>1.230046</td>\n",
       "      <td>1.088755</td>\n",
       "      <td>...</td>\n",
       "      <td>0.721638</td>\n",
       "      <td>0.635371</td>\n",
       "      <td>0.591946</td>\n",
       "      <td>0.595760</td>\n",
       "      <td>0.437298</td>\n",
       "      <td>0.492026</td>\n",
       "      <td>0.389003</td>\n",
       "      <td>0.307370</td>\n",
       "      <td>247.334466</td>\n",
       "      <td>0.043384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-56.407510</td>\n",
       "      <td>-72.715728</td>\n",
       "      <td>-33.680984</td>\n",
       "      <td>-5.519697</td>\n",
       "      <td>-42.147898</td>\n",
       "      <td>-26.160506</td>\n",
       "      <td>-31.764946</td>\n",
       "      <td>-73.216718</td>\n",
       "      <td>-9.283925</td>\n",
       "      <td>...</td>\n",
       "      <td>-34.830382</td>\n",
       "      <td>-10.933144</td>\n",
       "      <td>-44.807735</td>\n",
       "      <td>-2.836627</td>\n",
       "      <td>-10.295397</td>\n",
       "      <td>-2.534330</td>\n",
       "      <td>-22.565679</td>\n",
       "      <td>-11.710896</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>37912.750000</td>\n",
       "      <td>-1.020760</td>\n",
       "      <td>-0.564561</td>\n",
       "      <td>0.170073</td>\n",
       "      <td>-0.714009</td>\n",
       "      <td>-0.903653</td>\n",
       "      <td>-0.662022</td>\n",
       "      <td>-0.603820</td>\n",
       "      <td>-0.131071</td>\n",
       "      <td>-0.714753</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.226206</td>\n",
       "      <td>-0.548060</td>\n",
       "      <td>-0.171763</td>\n",
       "      <td>-0.324841</td>\n",
       "      <td>-0.136182</td>\n",
       "      <td>-0.326158</td>\n",
       "      <td>-0.060305</td>\n",
       "      <td>-0.004172</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>53665.500000</td>\n",
       "      <td>-0.269833</td>\n",
       "      <td>0.104206</td>\n",
       "      <td>0.750038</td>\n",
       "      <td>0.167473</td>\n",
       "      <td>-0.314849</td>\n",
       "      <td>-0.176600</td>\n",
       "      <td>-0.064160</td>\n",
       "      <td>0.080302</td>\n",
       "      <td>-0.154499</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059815</td>\n",
       "      <td>-0.095518</td>\n",
       "      <td>-0.044999</td>\n",
       "      <td>0.068815</td>\n",
       "      <td>0.166593</td>\n",
       "      <td>-0.064948</td>\n",
       "      <td>0.011781</td>\n",
       "      <td>0.023609</td>\n",
       "      <td>23.920000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>69322.000000</td>\n",
       "      <td>1.157985</td>\n",
       "      <td>0.776185</td>\n",
       "      <td>1.363041</td>\n",
       "      <td>0.993562</td>\n",
       "      <td>0.237514</td>\n",
       "      <td>0.465404</td>\n",
       "      <td>0.409714</td>\n",
       "      <td>0.374985</td>\n",
       "      <td>0.482352</td>\n",
       "      <td>...</td>\n",
       "      <td>0.113587</td>\n",
       "      <td>0.301082</td>\n",
       "      <td>0.083271</td>\n",
       "      <td>0.408740</td>\n",
       "      <td>0.418787</td>\n",
       "      <td>0.287195</td>\n",
       "      <td>0.087053</td>\n",
       "      <td>0.077127</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>83479.000000</td>\n",
       "      <td>1.960497</td>\n",
       "      <td>18.902453</td>\n",
       "      <td>9.382558</td>\n",
       "      <td>16.715537</td>\n",
       "      <td>34.801666</td>\n",
       "      <td>22.529298</td>\n",
       "      <td>36.677268</td>\n",
       "      <td>20.007208</td>\n",
       "      <td>15.594995</td>\n",
       "      <td>...</td>\n",
       "      <td>27.202839</td>\n",
       "      <td>10.503090</td>\n",
       "      <td>19.002942</td>\n",
       "      <td>4.022866</td>\n",
       "      <td>5.541598</td>\n",
       "      <td>3.517346</td>\n",
       "      <td>12.152401</td>\n",
       "      <td>33.847808</td>\n",
       "      <td>19656.530000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time             V1             V2             V3  \\\n",
       "count  140000.000000  140000.000000  140000.000000  140000.000000   \n",
       "mean    51858.089636      -0.249409       0.017429       0.672713   \n",
       "std     20867.521978       1.816521       1.614340       1.268657   \n",
       "min         0.000000     -56.407510     -72.715728     -33.680984   \n",
       "25%     37912.750000      -1.020760      -0.564561       0.170073   \n",
       "50%     53665.500000      -0.269833       0.104206       0.750038   \n",
       "75%     69322.000000       1.157985       0.776185       1.363041   \n",
       "max     83479.000000       1.960497      18.902453       9.382558   \n",
       "\n",
       "                  V4             V5             V6             V7  \\\n",
       "count  140000.000000  140000.000000  140000.000000  140000.000000   \n",
       "mean        0.139812      -0.282655       0.078898      -0.117062   \n",
       "std         1.322410       1.307926       1.284004       1.166853   \n",
       "min        -5.519697     -42.147898     -26.160506     -31.764946   \n",
       "25%        -0.714009      -0.903653      -0.662022      -0.603820   \n",
       "50%         0.167473      -0.314849      -0.176600      -0.064160   \n",
       "75%         0.993562       0.237514       0.465404       0.409714   \n",
       "max        16.715537      34.801666      22.529298      36.677268   \n",
       "\n",
       "                  V8             V9  ...            V21            V22  \\\n",
       "count  140000.000000  140000.000000  ...  140000.000000  140000.000000   \n",
       "mean        0.065205      -0.092188  ...      -0.039503      -0.118547   \n",
       "std         1.230046       1.088755  ...       0.721638       0.635371   \n",
       "min       -73.216718      -9.283925  ...     -34.830382     -10.933144   \n",
       "25%        -0.131071      -0.714753  ...      -0.226206      -0.548060   \n",
       "50%         0.080302      -0.154499  ...      -0.059815      -0.095518   \n",
       "75%         0.374985       0.482352  ...       0.113587       0.301082   \n",
       "max        20.007208      15.594995  ...      27.202839      10.503090   \n",
       "\n",
       "                 V23            V24            V25            V26  \\\n",
       "count  140000.000000  140000.000000  140000.000000  140000.000000   \n",
       "mean       -0.033419       0.012095       0.130218       0.023580   \n",
       "std         0.591946       0.595760       0.437298       0.492026   \n",
       "min       -44.807735      -2.836627     -10.295397      -2.534330   \n",
       "25%        -0.171763      -0.324841      -0.136182      -0.326158   \n",
       "50%        -0.044999       0.068815       0.166593      -0.064948   \n",
       "75%         0.083271       0.408740       0.418787       0.287195   \n",
       "max        19.002942       4.022866       5.541598       3.517346   \n",
       "\n",
       "                 V27            V28         Amount          Class  \n",
       "count  140000.000000  140000.000000  140000.000000  140000.000000  \n",
       "mean        0.000651       0.002244      91.210270       0.001886  \n",
       "std         0.389003       0.307370     247.334466       0.043384  \n",
       "min       -22.565679     -11.710896       0.000000       0.000000  \n",
       "25%        -0.060305      -0.004172       6.000000       0.000000  \n",
       "50%         0.011781       0.023609      23.920000       0.000000  \n",
       "75%         0.087053       0.077127      81.000000       0.000000  \n",
       "max        12.152401      33.847808   19656.530000       1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Fraud %  99.81\n",
      "\n",
      "count    139736.00\n",
      "mean         91.16\n",
      "std         247.34\n",
      "min           0.00\n",
      "25%           6.02\n",
      "50%          23.94\n",
      "75%          80.92\n",
      "max       19656.53\n",
      "Name: Amount, dtype: float64\n",
      "\n",
      "\n",
      "Fraud %     0.19\n",
      "\n",
      "count     264.00\n",
      "mean      115.39\n",
      "std       245.19\n",
      "min         0.00\n",
      "25%         1.00\n",
      "50%         9.56\n",
      "75%        99.99\n",
      "max      1809.68\n",
      "Name: Amount, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print ('Not Fraud % ',round(data['Class'].value_counts()[0]/len(data)*100,2))\n",
    "print ()\n",
    "print (round(data.Amount[data.Class == 0].describe(),2))\n",
    "print ()\n",
    "print ()\n",
    "print ('Fraud %    ',round(data['Class'].value_counts()[1]/len(data)*100,2))\n",
    "print ()\n",
    "print (round(data.Amount[data.Class == 1].describe(),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have a total of 140000 samples in this dataset. The PCA components (V1-V28) look as if they have similar spreads and rather small mean values in comparison to another predictors such as 'Time'. The majority (75%) of transactions are below 81 euros with some considerably high outliers (the max is 19656.53 euros). Around 0.19% of all the observed transactions were found to be fraudulent which means that we are dealing with an extremely unbalanced dataset. An important characteristic of such problems. Although the share may seem small, each fraud transaction can represent a very significant expense, which together can represent billions of dollars of lost revenue each year.\n",
    "### The next step is to defind our predictors and target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset in features and target variable\n",
    "y = data.Class # Target variable\n",
    "X = data.loc[:, data.columns != \"Class\"] # Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The next step would be to split our dataset and define the training and testing sets. The random seed (np.random.seed) is used to ensure that the same data is used for all runs. Let's do a 70/30 split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X and y into training and testing sets\n",
    "np.random.seed(123)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now it is time for model development and prediction! \n",
    "### import the Logistic Regression module and create a Logistic Regression classifier object using LogisticRegression() function. Then, fit your model on the train set using fit() and perform prediction on the test set using predict()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=10000,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the class\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# instantiate the model (using the default parameters)\n",
    "#logreg = LogisticRegression()\n",
    "logreg = LogisticRegression(solver='lbfgs',max_iter=10000)\n",
    "# fit the model with data  -TRAIN the model\n",
    "logreg.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST the model\n",
    "y_pred=logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Once the model and the predictions are ready, we can assess the performance of our classifier. First, we need to get our confusion matrix:\n",
    "\n",
    "*A confusion matrix is a table that is used to evaluate the performance of a classification model. You can also visualize the performance of an algorithm. The fundamental of a confusion matrix is the number of correct and incorrect predictions are summed up class-wise.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[34913    33]\n",
      " [   16    38]]\n",
      "True Positive Cases are 34913\n",
      "True Negative Cases are 16\n",
      "False Positive Cases are 33\n",
      "False Negative Cases are 38\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "cnf_matrix = metrics.confusion_matrix(y_pred, y_test)\n",
    "print(cnf_matrix)\n",
    "tpos = cnf_matrix[0][0]\n",
    "fneg = cnf_matrix[1][1]\n",
    "fpos = cnf_matrix[0][1]\n",
    "tneg = cnf_matrix[1][0]\n",
    "print(\"True Positive Cases are\",tpos) #How many non-fraud cases were identified as non-fraud cases - GOOD\n",
    "print(\"True Negative Cases are\",tneg) #How many Fraud cases were identified as Fraud cases - GOOD\n",
    "print(\"False Positive Cases are\",fpos) #How many Fraud cases were identified as non-fraud cases - BAD | (type 1 error)\n",
    "print(\"False Negative Cases are\",fneg) #How many non-fraud cases were identified as Fraud cases - BAD | (type 2 error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 257.44, 'Actual label')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_names=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Predicted label')\n",
    "plt.xlabel('Actual label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We should go further and evaluate the model using model evaluation metrics such as accuracy, precision, and recall. These are calculated based on the confustion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9986\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That is a fantastic accuracy score, isn't it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7037037037037037\n",
      "Recall: 0.5352112676056338\n",
      "F1-score: 0.608\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))\n",
    "print(\"F1-score:\",metrics.f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     34929\n",
      "           1       0.70      0.54      0.61        71\n",
      "\n",
      "    accuracy                           1.00     35000\n",
      "   macro avg       0.85      0.77      0.80     35000\n",
      "weighted avg       1.00      1.00      1.00     35000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Although the accuracy is excellent, the model struggles with fraud detection and has not captured about 30 out of 71 fraudulent transactions.\n",
    "### Accuracy in a highly unbalanced data set does not represent a correct value for the efficiency of a model. That's where precision, recall and more specifically F1-score as their combinations becomes important:\n",
    "\n",
    "- *Accuracy is used when the True Positives and True negatives are more important while F1-score is used when the False Negatives and False Positives are crucial*\n",
    "\n",
    "- *Accuracy can be used when the class distribution is similar while F1-score is a better metric when there are imbalanced classes as in the above case.*\n",
    "\n",
    "- *In most real-life classification problems, imbalanced class distribution exists and thus F1-score is a better metric to evaluate our model on.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://memegenerator.net/img/instances/80692979.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
