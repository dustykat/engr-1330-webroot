{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=gray>MAR 23rd, 2022</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=darkred>Laboratory 22: \"Reject it or Fail!\" or a Lab on \"Hypothesis Testing\" </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preamble script block to identify host, user, and kernel\n",
    "import sys\n",
    "! hostname\n",
    "! whoami\n",
    "print(sys.executable)\n",
    "print(sys.version)\n",
    "print(sys.version_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full name: \n",
    "## R#: \n",
    "## Date: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remember where we left our last laboratory session? \n",
    "\n",
    "![](https://media.tumblr.com/tumblr_mbui1kpPoU1rxdw8g.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accept my gratitude if you do! But in case you saw Agent K and Agent J sometime after Thursday or for any other reason, do not recall it, here is where were we left things:\n",
    "#### We had a dataset with two sets of numbers (Set 1 and Set2). We did a bunch of stuff and decided that the Normal Distribution Data Model provides a good fit for both of sample sets. We, then used the right parameters for Normal Data Model (mean and standard deviation) to generate one new sample set based on each set. We then looked at the four sets next to each other and asked a rather simple question: Are these sets different or similar?\n",
    "#### While we reached some assertions based on visual assessment, we did not manage to solidify our assertation in any numerical way. Well, now is the time! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the necessary packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#Previously ...\n",
    "data = pd.read_csv(\"hypothesis_lab_data.csv\") \n",
    "set1 = np.array(data['Set1'])\n",
    "set2 = np.array(data['Set2'])\n",
    "mu1 = set1.mean()\n",
    "sd1 = set1.std()\n",
    "mu2 = set2.mean()\n",
    "sd2 = set2.std()\n",
    "set1_s = np.random.normal(mu1, sd1, 100)\n",
    "set2_s = np.random.normal(mu2, sd2, 100)\n",
    "data2 = pd.DataFrame({'Set1s':set1_s,'Set2s':set2_s})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#Previously ...\n",
    "fig, ax = plt.subplots()\n",
    "data2.plot.hist(density=False, ax=ax, title='Histogram: Set1 and Set1 samples vs. Set2 and Set2 samples', bins=40)\n",
    "data.plot.hist(density=False, ax=ax, bins=40)\n",
    "\n",
    "ax.set_ylabel('Count')\n",
    "ax.grid(axis='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#Previously ...\n",
    "fig = plt.figure(figsize =(10, 7)) \n",
    "plt.boxplot ([set1, set1_s, set2, set2_s],1, '')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__We can use statistical hypothesis tests to confirm that our sets are from Normal Distribution Data Models. We can use the Shapiro-Wilk Normality Test:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# the Shapiro-Wilk Normality Test for set1\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "stat, p = shapiro(data['Set1'])\n",
    "print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "if p > 0.05:\n",
    "\tprint('Probably Gaussian')\n",
    "else:\n",
    "\tprint('Probably not Gaussian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# the Shapiro-Wilk Normality Test for set2\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "stat, p = shapiro(data['Set2'])\n",
    "print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "if p > 0.05:\n",
    "\tprint('Probably Gaussian')\n",
    "else:\n",
    "\tprint('Probably not Gaussian')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Now let's confirm that set1 and set1_s are from the same distribution. We can use the Mann-Whitney U Test for this:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu # import a useful non-parametric test\n",
    "stat, p = mannwhitneyu(data['Set1'],data2['Set1s'])\n",
    "print('statistic=%.3f, p-value at rejection =%.3f' % (stat, p))\n",
    "if p > 0.05:\n",
    "    print('Probably the same distribution')\n",
    "else:\n",
    "    print('Probably different distributions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Let's also confirm that set2 and set2_s are from the same distribution:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu # import a useful non-parametric test\n",
    "stat, p = mannwhitneyu(data['Set2'],data2['Set2s'])\n",
    "print('statistic=%.3f, p-value at rejection =%.3f' % (stat, p))\n",
    "if p > 0.05:\n",
    "    print('Probably the same distribution')\n",
    "else:\n",
    "    print('Probably different distributions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Based on the results we can say set1 and set1_s probably belong to the same distrubtion. The same can be stated about set2 and set2_s. Now let's check and see if set1 and set2 are SIGNIFICANTLY different or not?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu # import a useful non-parametric test\n",
    "stat, p = mannwhitneyu(data['Set1'],data['Set2'])\n",
    "print('statistic=%.3f, p-value at rejection =%.3f' % (stat, p))\n",
    "if p > 0.05:\n",
    "    print('Probably the same distribution')\n",
    "else:\n",
    "    print('Probably different distributions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The test's result indicate that the set1 and set2 belong to distirbutions with different measures of central tendency (means). We can check the same for set1_s and set2_s as well:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu # import a useful non-parametric test\n",
    "stat, p = mannwhitneyu(data2['Set1s'],data2['Set2s'])\n",
    "print('statistic=%.3f, p-value at rejection =%.3f' % (stat, p))\n",
    "if p > 0.05:\n",
    "    print('Probably the same distribution')\n",
    "else:\n",
    "    print('Probably different distributions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Now we can state at a 95% confidence level that set1 and set2 are different. The same for set1s and set2s.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Example: A dataset containing marks obtained by students on basic skills like basic math and language skills (reading and writing) is collected from an educational institution and we have been tasked to give them some important inferences. \n",
    "\n",
    "> Hypothesis: There is no difference in means of student performance in any of basic literacy skills i.e. reading, writing, math.\n",
    "\n",
    "___\n",
    "*This is based on an example by Joju John Varghese on \"Hypothesis Testing for Inference using a Dataset\" available @ https://medium.com/swlh/hypothesis-testing-for-inference-using-a-data-set-aaa799e94cdf. The dataset is available @ https://www.kaggle.com/spscientist/students-performance-in-exams.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"StudentsPerformance.csv\") \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set1 = df['math score']\n",
    "set2 = df['reading score']\n",
    "set3 = df['writing score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.distplot(set1,color='navy', rug=True)\n",
    "sns.distplot(set2,color='darkorange', rug=True)\n",
    "sns.distplot(set3,color='green', rug=True)\n",
    "plt.xlim(0,100)\n",
    "plt.xlabel('Test Results')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  It seems that all three samples have the same population means and it seems there is no significant difference between them at all. Let's set the null and alternative hypothesis:\n",
    "\n",
    "> Ho: There is no difference in performance of students between math, reading and writing skills. <br>\n",
    "Ha: There is a difference in performance of students between math, reading and writing skills. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu # import a useful non-parametric test\n",
    "stat, p = mannwhitneyu(set1,set2)\n",
    "print('statistic=%.3f, p-value at rejection =%.3f' % (stat, p))\n",
    "if p > 0.05:\n",
    "    print('Probably the same distribution')\n",
    "else:\n",
    "    print('Probably different distributions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu # import a useful non-parametric test\n",
    "stat, p = mannwhitneyu(set1,set3)\n",
    "print('statistic=%.3f, p-value at rejection =%.3f' % (stat, p))\n",
    "if p > 0.05:\n",
    "    print('Probably the same distribution')\n",
    "else:\n",
    "    print('Probably different distributions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu # import a useful non-parametric test\n",
    "stat, p = mannwhitneyu(set2,set3)\n",
    "print('statistic=%.3f, p-value at rejection =%.3f' % (stat, p))\n",
    "if p > 0.05:\n",
    "    print('Probably the same distribution')\n",
    "else:\n",
    "    print('Probably different distributions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kruskal\n",
    "stat, p = kruskal(set1, set2, set3)\n",
    "print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "# interpret\n",
    "alpha = 0.05\n",
    "if p > 0.05:\n",
    "    print('Probably the same distribution')\n",
    "else:\n",
    "    print('Probably different distributions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Website Design: A practical example of A/B Testing\n",
    "*inspired by an example in __\"A/B Test Significance in Python\"__ by __Samuel Hinton__ available at* https://cosmiccoding.com.au/tutorials/ab_tests <br>\n",
    "![](https://www.invespcro.com/blog/images/blog-images/ab-test-1-1.jpg) <br>\n",
    "#### Imagine you’re in charge of a website (e.g., an online videogame shop). You have the current version of the website (aka. \"A\"), but aren’t happy with it. For instance, you are not selling as much as you like. You want to change the design of the \"Add to Cart\" button (aka. \"B\") and maybe that will increase your sells. <br>\n",
    "![](https://www.volusion.com/blog/content/images/wp/buttonaandbuttonn.jpg) <br>\n",
    "\n",
    "#### you set up your website so that half the people are directed to the old website, and half to one where you’ve made your change. You have data from both, and want to know, with confidence, “Does the change I made increase the sells?”.<br>\n",
    "\n",
    "*This is an A/B test. Often this is used interchangably with the term “split testing”, though in general A/B tests test small changes, and split testing might be when you present two entirely different websites to the user.* <br>\n",
    "\n",
    "#### Why not just change the website and monitor it for a week?\n",
    "    Good question - by having two sites active at once and randomly directing users to one or the other, you control for all other variables. If one week later puts you the week before Christmas, this will impact sales, and you might draw the wrong conclusion because of these confounding effects.\n",
    "#### Why is it not an A/B/C test?\n",
    "    you can have as many perturbations running as you want, but got to keep the name simple. The more perturbations you try though, the smaller a number of samples you’ll have for each case, and the harder it will be to draw statistically significant conclusions.\n",
    "\n",
    "#### Let us assume you have 1000 users, 550 were directed to site A, 450 to site B. In site A, 48 users made a purchase. In site B, 56 users made a purchase. Is this a statistically significant result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_a= 550\n",
    "num_b = 450\n",
    "click_a= 48\n",
    "click_b = 56\n",
    "rate_a= click_a / num_a\n",
    "rate_b = click_b / num_b\n",
    "print(rate_a)\n",
    "print(rate_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can click a button, or not. Two discrete options are available, so this is a textbook binomial distribution, with some unknown rate for site A and site B. We don’t know the true click rate, but we can estimate it using our small sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import binom\n",
    "\n",
    "# Determine the probability of having x number of clicks - Binomial Dist.\n",
    "clicks = np.arange(10, 100)\n",
    "prob_a = binom(num_a, rate_a).pmf(clicks)\n",
    "prob_b = binom(num_b, rate_b).pmf(clicks)\n",
    "\n",
    "# Make the bar plots.\n",
    "plt.bar(clicks, prob_a, label=\"A\", alpha=0.7)\n",
    "plt.bar(clicks, prob_b, label=\"B\", alpha=0.7)\n",
    "plt.legend()\n",
    "plt.xlabel(\"# of Sells\"); plt.ylabel(\"Probability\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So we can see here that b has an edge, but its certaintly possible if we pick two random points according to the histograms for A and B, that A might actually be higher than B! <Br>\n",
    "    \n",
    "#### As we’re interested in the average # of sells, this averaging of an underlying distribution means our final estimate will be well approximated by a normal distribution. So let’s reformulate, using the normal approximation here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "# Where does this come from? See this link: https://en.wikipedia.org/wiki/Binomial_distribution#Normal_approximation\n",
    "std_a = np.sqrt(rate_a * (1 - rate_a) / num_a)\n",
    "std_b = np.sqrt(rate_b * (1 - rate_b) / num_b)\n",
    "\n",
    "click_rate = np.linspace(0, 0.2, 200)\n",
    "prob_a = norm(rate_a, std_a).pdf(click_rate)\n",
    "prob_b = norm(rate_b, std_b).pdf(click_rate)\n",
    "\n",
    "# Make the bar plots.\n",
    "plt.plot(click_rate, prob_a, label=\"A\")\n",
    "plt.plot(click_rate, prob_b, label=\"B\")\n",
    "plt.legend(frameon=False)\n",
    "plt.xlabel(\"Purchase rate\"); plt.ylabel(\"Probability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is also a better plot than the first one, because we’ve removed the confusing effect of site A and site B having a slightly different number of visitors had. So our question is still the same: What is the chance that a draw from B is higher than a draw from A. Is it significant? <br>\n",
    "#### To answer this, let us utilise the handy fact that the sum (or difference) of normally distributed random numbers is also a normal. This is simple - take the difference in the means and sum the variance. We’ll do two things below: First, get the z-score, and second, plot the proper distribution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_score = (rate_b - rate_a) / np.sqrt(std_a**2 + std_b**2)\n",
    "p = norm(rate_b - rate_a, np.sqrt(std_a**2 + std_b**2))\n",
    "\n",
    "x = np.linspace(-0.05, 0.15, 1000)\n",
    "y = p.pdf(x)\n",
    "area_under_curve = p.sf(0)\n",
    "plt.plot(x, y, label=\"PDF\")\n",
    "plt.fill_between(x, 0, y, where=x>0, label=\"Prob(b>a)\", alpha=0.3)\n",
    "plt.annotate(f\"Area={area_under_curve:0.3f}\", (0.02, 5))\n",
    "plt.legend()\n",
    "plt.xlabel(\"Difference in purchase rate\"); plt.ylabel(\"Prob\");\n",
    "\n",
    "print(f\"zscore is {z_score:0.3f}, with p-value {norm().sf(z_score):0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we can say that given the null hypothesis (\"B is less than or equal to A\") is true , we would expect to get this result or a result more extreme only 2.9% of the time. As that is a significant result (typically p < 5%), we reject the null hypothesis, and state that we have evidence that B > A. <br>\n",
    "#### we’ve made a lot of plots for this to try and explain the concept. You can easily write a tiny function to simplify all of this. Whether you want the confidence or the p-value just means changing the final \"norm.cdf\" to \"norm.sf\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confidence_ab_test(click_a, num_a, click_b, num_b):\n",
    "    rate_a = click_a / num_a\n",
    "    rate_b = click_b / num_b\n",
    "    std_a = np.sqrt(rate_a * (1 - rate_a) / num_a)\n",
    "    std_b = np.sqrt(rate_b * (1 - rate_b) / num_b)\n",
    "    z_score = (rate_b - rate_a) / np.sqrt(std_a**2 + std_b**2)\n",
    "    return norm.sf(z_score)\n",
    "\n",
    "print(get_confidence_ab_test(click_a, num_a, click_b, num_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remember Non-parametric Statistical Hypothesis Tests? We can use them here as well! <br>\n",
    "#### Imagine we have the raw results of clicks (purchases), as 0s or 1s, as our distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "a_dist = np.zeros(num_a)\n",
    "a_dist[:click_a] = 1\n",
    "b_dist = np.zeros(num_b)\n",
    "b_dist[:click_b] = 1\n",
    "\n",
    "stat, p_value = mannwhitneyu(a_dist, b_dist, alternative=\"less\")\n",
    "print(f\"Mann-Whitney U test for null hypothesis B <= A is {p_value:0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "![](https://media2.giphy.com/media/5nj4ZZWl6QwneEaBX4/source.gif) <br>\n",
    "\n",
    "\n",
    "__*Here are some great reads on this topic:*__ \n",
    "- __\"Hypothesis testing in Machine learning using Python\"__ by __Yogesh Agrawal__ available at *https://towardsdatascience.com/hypothesis-testing-in-machine-learning-using-python-a0dc89e169ce* <br>\n",
    "- __\"Quick Guide To Perform Hypothesis Testing\"__ available at *https://www.analyticsvidhya.com/blog/2020/12/quick-guide-to-perform-hypothesis-testing/*<br>\n",
    "- __\"A Gentle Introduction to Statistical Hypothesis Testing\"__ by __Jason Brownlee__ available at *https://machinelearningmastery.com/statistical-hypothesis-tests/*<br>\n",
    "- __\"17 Statistical Hypothesis Tests in Python (Cheat Sheet)\"__ by __Jason Brownlee__ available at *https://machinelearningmastery.com/statistical-hypothesis-tests-in-python-cheat-sheet/*<br>\n",
    "-  __\"Implementing A/B Tests in Python\"__ by __Robbie Geoghegan__ available at* https://medium.com/@robbiegeoghegan/implementing-a-b-tests-in-python-514e9eb5b3a1 <br>\n",
    "-  __\"The Math Behind A/B Testing with Example Python Code\"__ by __Nguyen Ngo__ available at* https://towardsdatascience.com/the-math-behind-a-b-testing-with-example-code-part-1-of-2-7be752e1d06f <br>\n",
    "-  __\"A/B Testing\"__ available at* https://www.optimizely.com/optimization-glossary/ab-testing/ <br>\n",
    "-  __\"A/B Testing Guide\"__ available at* https://vwo.com/ab-testing/ <br>\n",
    "\n",
    "__*Some great videos:*__\n",
    "-  __\"What is A/B Testing? | Data Science in Minutes\"__ by __Data Science Dojo__ available at* https://www.youtube.com/watch?v=zFMgpxG-chM <br>\n",
    "-  __\"A/B Testing Intro: Why, What, Where, & How to A/B Test\"__ by __Testing Theory__ available at* https://www.youtube.com/watch?v=CH89jd4haRE <br>\n",
    "-  __\"A/B Testing\"__ by __Udacity__ available at* https://www.youtube.com/watch?v=8H6QmMQWPEI <br>\n",
    "- __\"Statistical Hypothesis Testing- Data Science with Python\"__ by __Technology for Noobs__ available at *https://www.youtube.com/watch?v=kd6zKBa9Rfk* <br>\n",
    "- __\"Hypothesis Testing, p-value & Confidence Intervals, Exploratory Data Analysis In Python Statistics\"__ by __TheEngineeringWorld__ available at *https://www.youtube.com/watch?v=kz1IXqcFVCo* <br>\n",
    "- __\"Python Tutorial : Hypothesis tests\"__ by __DataCamp__ available at *https://www.youtube.com/watch?v=6wbldEMpRXc* <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "![](https://media.csesoc.org.au/content/images/2019/10/learn11.gif) <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Wait a minute ... Isn't The Kruskal-Wallis test missing something?  <br>\n",
    "\n",
    "### We used the Kruskal-Wallis to check whether the three sets belong to the same distribution or at least one of them is different. The question is, how can we find the sets that are different? What is the missing piece in tests such as The Kruskal-Wallis Test?  \n",
    "\n",
    "#### _Make sure to cite any resources that you may use._ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'> Explain here... </font> <br>\n",
    "<font color='green'> Explain here... </font> <br>\n",
    "<font color='green'> Explain here... </font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://img.picturequotes.com/2/124/123499/one-finds-the-truth-by-making-a-hypothesis-and-comparing-observations-with-the-hypothesis-quote-1.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
