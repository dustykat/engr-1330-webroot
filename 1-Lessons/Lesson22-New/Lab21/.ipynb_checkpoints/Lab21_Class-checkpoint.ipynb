{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratory 21: Linear Regression | Intervals, Errors, Metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full name: \n",
    "## R#: \n",
    "## HEX: \n",
    "## Title of the notebook\n",
    "## Date: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://live.staticflickr.com/4028/4636032975_df7f27eac9_b.jpg) <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last week, we talked about linear regression ... <br>\n",
    "\n",
    "![](https://miro.medium.com/max/888/1*guak1sQTh5sAf46NMzbQig.jpeg) <br>\n",
    "\n",
    "\n",
    "- __What is linear regression?__<br>\n",
    "    a basic predictive analytics technique that uses historical data to predict an output variable.<br>\n",
    "- __Why do we need linear regression?__\n",
    "    To explore the relationship between predictor and output variables and predict the output variable based on known values of predictors.  <br>    \n",
    "- __How does linear regression work?__\n",
    "    To estimate Y using linear regression, we assume the equation:  ùëåùëí=Œ≤ùëã+Œ± <br>\n",
    "    Our goal is to find statistically significant values of the parameters Œ± and Œ≤ that minimise the difference between Y and Y‚Çë. If we are able to determine the optimum values of these two parameters, then we will have the line of best fit that we can use to predict the values of Y, given the value of X. <br>\n",
    "- __How to estimate the coefficients?__\n",
    "    We have used \"Ordinary Least Squares (OLS)\" and \"Maximum Likelihood Estimation (MLE)\" methods. We can get formulas for the slope and intercept of the line of best fit from each method. Once we have the equation of the line of best fit, we can use it to fit a line and assess the quality of fit as well as predicting.\n",
    "- __How to assess the fit?__\n",
    "    We have used graphs and visual assessments to describe the fits, identify regions with more and less errors, and decide whether the fit is trustworthy or not. \n",
    "![](https://miro.medium.com/max/683/1*h6PuI6-PdPE8d4dTnhcg3w.png) <br>\n",
    "    We also use \"Goodness-of-Fit (GOF)\" metrics to describe the errors and performance of the linear models.   \n",
    "\n",
    "- __How confident are we with a prediction?__\n",
    "    By definition, the prediction of a linear regression model is an estimate or an approximation and contains some uncertainty. The uncertainty comes from the errors in the model itself and noise in the input data. The model is an approximation of the relationship between the input variables and the output variables. The model error can be decomposed into three sources of error: the variance of the model, the bias of the model, and the variance of the irreducible error (the noise) in the data.\n",
    "\n",
    "        Error(Model) = Variance(Model) + Bias(Model) + Variance(Irreducible Error)\n",
    "    \n",
    "    Before going any further, let's assume that you were arrested by the king's guard as you were minding your business in the streets of King's Landing for the crime of planning for the murder of King Joffrey Baratheon. As much as you hate King Joffrey you had no plans for killing him but no one believes you. In the absence of witnesses or a confession, you demand trial by combat. ![](https://www.reactiongifs.com/r/trial-by-combat.gif) But they inform you that the Germanic law to settle accusations is no longer used and it has been replaced with a new method. You get to choose a bowman. That bowman will make 3 shots for you. And if he hits the bullseye you will walk a free man. Otherwise, you will be hanged. ![](https://i.imgur.com/kqsurCZ.gif) \n",
    "    You have two options. The first bowman is Horace. He is known as one of the greatest target archers of all time. He is old though and due to lack of an efficient social security system in Westeros, he has to work as a hired bowman for the high court to earn a living. You ask around and you hear that he still can shoot a bullseye but as his hands shake, he sometimes misses by a lot. The second archer is Daryl. He is also a wellkown archer but unfortunately he has a drinking problem. You have understood that there has been cases that he has shot the bullseye in all of his three shots and there has been cases that he has completely missed the bullseye. The thing about him is that his three shots are always very close together. Now, you get to pick. Between Horace and Daryl, who would you choose to shoot for your freedom?   \n",
    "\n",
    "- __Bias, Variance, and the bowman dilemma!__\n",
    "    We used the example above to give you an initial understanding of bias and variance and their impact on a model's performance. Given this is a complicated and yet important aspect of data modeling and machine learning, without getting into too much detail, we will discuss these concepts. Bias reflects how close the functional form of the model can get to the true relationship between the predictors and the outcome. Variance refers to the amount by which [the model] would change if we estimated it using a different training data set.  ![](https://miro.medium.com/max/1670/1*On4Uk9Favg50ylBOak-ECQ@2x.png) Looking at the picture above, Horace was an archer with high variance and low bias, while Daryl had high bias and low variability. In an ideal world, we want low bias and low variance which we cannot have. When there is a high bias error, it results in a very simplistic model that does not consider the variations very well. Since it does not learn the training data very well, it is called Underfitting. When the model has a high variance, it will still consider the noise as something to learn from. That is, the model learns the noise from the training data, hence when confronted with new (testing) data, it is unable to predict accurately based on it. Since in the case of high variance, the model learns too much from the training data, it is called overfitting. To summarise:\n",
    "    - A model with a high bias error underfits data and makes very simplistic assumptions on it\n",
    "    - A model with a high variance error overfits the data and learns too much from it\n",
    "    - A good model is where both Bias and Variance errors are balanced. The balance between the Bias error and the Variance error is the Bias-Variance Tradeoff.\n",
    "    \n",
    "    The irreducible error is the error that we can not remove with our model, or with any model. The error is caused by elements outside our control, such as statistical noise in the observations. A model with low bias and high variance predicts points that are around the center generally, but pretty far away from each other (Horace). A model with high bias and low variance is pretty far away from the bull‚Äôs eye, but since the variance is low, the predicted points are closer to each other (Daryl). Bias and Variance play an important role in deciding which predictive model to use: Something that you will definitly learn more about if you go further in the field of machine learning and predicitve models.\n",
    "\n",
    "- __How can we measure bias and variance?__\n",
    "    \n",
    "    There are GOF metrics that can measure the bias and variance of a model: For example the Nash‚ÄìSutcliffe model efficiency coefficient and the Kling-Gupta Efficiency (KGE). The Nash‚ÄìSutcliffe efficiency is calculated as one minus the ratio of the error variance of the modeled time-series divided by the variance of the observed time-series. In the situation of a perfect model with an estimation error variance equal to zero, the resulting Nash-Sutcliffe Efficiency equals 1 (NSE = 1). KGE provides a diagnostically interesting decomposition of the Nash-Sutcliffe efficiency (and hence MSE), which facilitates the analysis of the relative importance of its different components (correlation, bias and variability).   \n",
    "    \n",
    "    \n",
    " <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Let's have a look at our old good example of TV, Radio, and Newspaper advertisements and number of sales for a specific product.! <br>\n",
    "\n",
    "#### Let's say that we are interested to compare the performance of the linear models that use TV spendings and Radio spendings as their predictor variables in terms of accuracy, bias, and variability. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statistics\n",
    "import scipy.stats\n",
    "from matplotlib import pyplot as plt\n",
    "import statsmodels.formula.api as smf\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "# Import and display first rows of the advertising dataset\n",
    "df = pd.read_csv('advertising.csv')\n",
    "tv = np.array(df['TV'])\n",
    "radio = np.array(df['Radio'])\n",
    "newspaper = np.array(df['Newspaper'])\n",
    "sales = np.array(df['Sales'])\n",
    "# Initialise and fit linear regression model using `statsmodels`\n",
    "# TV Spending as predictor\n",
    "model_tv = smf.ols('Sales ~ TV', data=df)\n",
    "model_tv = model_tv.fit()\n",
    "TV_pred = model_tv.predict()\n",
    "# Radio Spending as predictor\n",
    "model_rd = smf.ols('Sales ~ Radio', data=df)\n",
    "model_rd = model_rd.fit()\n",
    "RD_pred = model_rd.predict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for TV ad spendings as predictor is  3.2423221486546887\n",
      "RMSE for Radio ad spendings as predictor is  4.2535159274564185\n"
     ]
    }
   ],
   "source": [
    "print(\"RMSE for TV ad spendings as predictor is \",np.sqrt(metrics.mean_squared_error(sales, TV_pred)))\n",
    "print(\"RMSE for Radio ad spendings as predictor is \",np.sqrt(metrics.mean_squared_error(sales, RD_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 for TV ad spendings as predictor is  0.611875050850071\n",
      "R2 for Radio ad spendings as predictor is  0.33203245544529525\n"
     ]
    }
   ],
   "source": [
    "print(\"R2 for TV ad spendings as predictor is \",metrics.r2_score(sales, TV_pred))\n",
    "print(\"R2 for Radio ad spendings as predictor is \",metrics.r2_score(sales, RD_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson's r for TV ad spendings as predictor is  0.7822244248616065\n",
      "Pearson's for Radio ad spendings as predictor is  0.5762225745710552\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr \n",
    "tv_r = pearsonr(TV_pred, sales)\n",
    "rd_r = pearsonr(RD_pred, sales)\n",
    "\n",
    "print(\"Pearson's r for TV ad spendings as predictor is \",tv_r[0])\n",
    "print(\"Pearson's for Radio ad spendings as predictor is \",rd_r[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE for TV ad spendings as predictor is  [0.61187505]\n",
      "NSE for Radio ad spendings as predictor is  [0.33203246]\n"
     ]
    }
   ],
   "source": [
    "from hydroeval import *          #Notice this importing method\n",
    "tv_nse = evaluator(nse, TV_pred, sales)\n",
    "rd_nse = evaluator(nse, RD_pred, sales)\n",
    "\n",
    "print(\"NSE for TV ad spendings as predictor is \",tv_nse)\n",
    "print(\"NSE for Radio ad spendings as predictor is \",rd_nse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KGE for TV ad spendings as predictor is  [[0.69201883]\n",
      " [0.78222442]\n",
      " [0.78222442]\n",
      " [1.        ]]\n",
      "KGE for Radio ad spendings as predictor is  [[0.40068822]\n",
      " [0.57622257]\n",
      " [0.57622257]\n",
      " [1.        ]]\n"
     ]
    }
   ],
   "source": [
    "tv_kge = evaluator(kgeprime, TV_pred, sales)\n",
    "rd_kge = evaluator(kgeprime, RD_pred, sales)\n",
    "\n",
    "print(\"KGE for TV ad spendings as predictor is \",tv_kge)\n",
    "print(\"KGE for Radio ad spendings as predictor is \",rd_kge)\n",
    "#KGE: Kling-Gupta efficiencies range from -Inf to 1. Essentially, the closer to 1, the more accurate the model is.\n",
    "#r: the Pearson product-moment correlation coefficient. Ideal value is r=1\n",
    "#Gamma: the ratio between the coefficient of variation (CV) of the simulated values to \n",
    "       #the coefficient of variation of the observed ones. Ideal value is Gamma=1\n",
    "#Beta: the ratio between the mean of the simulated values and the mean of the observed ones. Ideal value is Beta=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __How confident are we with our linear regression model?__\n",
    "\n",
    "    The 95% confidence interval for the forecasted values ≈∑ of x is\n",
    "    \n",
    "    ![](https://i0.wp.com/www.real-statistics.com/wp-content/uploads/2012/12/confidence-interval-regression.png?resize=92%2C20&ssl=1) \n",
    "    \n",
    "    where \n",
    "    \n",
    "    ![](https://i2.wp.com/www.real-statistics.com/wp-content/uploads/2013/02/image1773.png?w=154&ssl=1)   \n",
    "    \n",
    "    This means that there is a 95% probability that the true linear regression line of the population will lie within the confidence interval of the regression line calculated from the sample data.\n",
    "    \n",
    "    ![](https://i1.wp.com/www.real-statistics.com/wp-content/uploads/2012/12/confidence-prediction-interval.png?w=860&ssl=1)\n",
    "    \n",
    "    In the graph on the left of Figure 1, a linear regression line is calculated to fit the sample data points. The confidence interval consists of the space between the two curves (dotted lines). Thus there is a 95% probability that the true best-fit line for the population lies within the confidence interval (e.g. any of the lines in the figure on the right above).\n",
    "    \n",
    "    There is also a concept called a prediction interval. Here we look at any specific value of x, x0, and find an interval around the predicted value ≈∑0 for x0 such that there is a 95% probability that the real value of y (in the population) corresponding to x0 is within this interval (see the graph on the right side). The 95% prediction interval of the forecasted value ≈∑0 for x0 is \n",
    "    \n",
    "    ![](https://i1.wp.com/www.real-statistics.com/wp-content/uploads/2012/12/prediction-interval-regression.png?resize=98%2C20&ssl=1)\n",
    "    \n",
    "    where the standard error of the prediction is\n",
    "    \n",
    "    ![](https://i0.wp.com/www.real-statistics.com/wp-content/uploads/2012/12/standard-error-prediction.png?resize=186%2C55&ssl=1)\n",
    "    \n",
    "    For any specific value x0 the prediction interval is more meaningful than the confidence interval.\n",
    "    \n",
    "    ![](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2018/04/Relationship-between-prediction-actual-value-and-prediction-interval.png)\n",
    "    \n",
    " <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Let's work on another familier example.  <br>\n",
    "#### We had a table of recoded times and speeds from some experimental observations:\n",
    "\n",
    "|Elapsed Time (s)|Speed (m/s)|\n",
    "|---:|---:|\n",
    "|0 |0|\n",
    "|1.0 |3|\n",
    "|2.0 |7|\n",
    "|3.0 |12|\n",
    "|4.0 |20|\n",
    "|5.0 |30|\n",
    "|6.0 | 45.6| \n",
    "|7.0 | 60.3 |\n",
    "|8.0 | 77.7 |\n",
    "|9.0 | 97.3 |\n",
    "|10.0| 121.1|\n",
    "\n",
    "#### This time we want to explore the confidence and prediciton intervals for our linear regression model: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.929\n",
      "Model:                            OLS   Adj. R-squared:                  0.922\n",
      "Method:                 Least Squares   F-statistic:                     118.6\n",
      "Date:                Sun, 08 Nov 2020   Prob (F-statistic):           1.75e-06\n",
      "Time:                        12:50:35   Log-Likelihood:                -41.405\n",
      "No. Observations:                  11   AIC:                             86.81\n",
      "Df Residuals:                       9   BIC:                             87.61\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const        -16.7864      6.507     -2.580      0.030     -31.507      -2.066\n",
      "x1            11.9773      1.100     10.889      0.000       9.489      14.465\n",
      "==============================================================================\n",
      "Omnibus:                        1.397   Durbin-Watson:                   0.386\n",
      "Prob(Omnibus):                  0.497   Jarque-Bera (JB):                0.993\n",
      "Skew:                           0.508   Prob(JB):                        0.609\n",
      "Kurtosis:                       1.934   Cond. No.                         11.3\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[-16.78636364  11.97727273]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Farha\\Anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1450: UserWarning: kurtosistest only valid for n>=20 ... continuing anyway, n=11\n",
      "  \"anyway, n=%i\" % int(n))\n"
     ]
    }
   ],
   "source": [
    "time = [0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]\n",
    "speed = [0, 3, 7, 12, 20, 30, 45.6, 60.3, 77.7, 97.3, 121.2]\n",
    "x = np.array(time)\n",
    "Y = np.array(speed)\n",
    "\n",
    "#We already know these parameters from last week but let's assume that we don't!\n",
    "# alpha = -16.78636363636364\n",
    "# beta = 11.977272727272727\n",
    "#Our linear model: ypred = alpha + beta * x\n",
    "\n",
    "import statsmodels.api as sm     #needed for linear regression\n",
    "from statsmodels.sandbox.regression.predstd import wls_prediction_std   #needed to get prediction interval\n",
    "X = sm.add_constant(x)\n",
    "re = sm.OLS(Y, X).fit()\n",
    "print(re.summary())\n",
    "print(re.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-46.74787932 -33.82587196 -21.09197931  -8.56161668   3.75287435\n",
      "  15.84348029  27.70741981  39.34747423  50.77165706  61.99230986\n",
      "  73.02484795]\n",
      "[ 13.17515205  24.20769014  35.42834294  46.85252577  58.49258019\n",
      "  70.35651971  82.44712565  94.76161668 107.29197931 120.02587196\n",
      " 132.94787932]\n"
     ]
    }
   ],
   "source": [
    "prstd, iv_l, iv_u = wls_prediction_std(re) #iv_l and iv_u give you the limits of the prediction interval for each point.\n",
    "print(iv_l)\n",
    "print(iv_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from statsmodels.stats.outliers_influence import summary_table\n",
    "\n",
    "st, data, ss2 = summary_table(re, alpha=0.05)\n",
    "\n",
    "fittedvalues = data[:, 2]\n",
    "predict_mean_se  = data[:, 3]\n",
    "predict_mean_ci_low, predict_mean_ci_upp = data[:, 4:6].T\n",
    "predict_ci_low, predict_ci_upp = data[:, 6:8].T\n",
    "\n",
    "\n",
    "plt.plot(x, Y, 'o')\n",
    "plt.plot(x, fittedvalues, '-',color='red', lw=2)\n",
    "#plt.plot(x, predict_ci_low, '--', color='green',lw=2) #Lower prediction band\n",
    "#plt.plot(x, predict_ci_upp, '--', color='green',lw=2) #Upper prediction band\n",
    "#plt.plot(x, predict_mean_ci_low,'--', color='orange',  lw=2) #Lower confidence band\n",
    "#plt.plot(x, predict_mean_ci_upp,'--', color='orange', lw=2) #Upper confidence band\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://media2.giphy.com/media/5nj4ZZWl6QwneEaBX4/source.gif) <br>\n",
    "\n",
    "*This notebook was inspired by a several blogposts including:* \n",
    "\n",
    "- __\"How to Calculate the Bias-Variance Trade-off with Python\"__ by __Jason Brownlee__ available at* https://machinelearningmastery.com/calculate-the-bias-variance-trade-off/ <br>\n",
    "- __\"Bias and Variance in Machine Learning ‚Äì A Fantastic Guide for Beginners!\"__ by __PURVA HUILGOL__ available at* https://www.analyticsvidhya.com/blog/2020/08/bias-and-variance-tradeoff-machine-learning/ <br>\n",
    "- __\"Prediction Intervals for Machine Learning\"__ by __Jason Brownlee__ available at* https://machinelearningmastery.com/prediction-intervals-for-machine-learning/ <br>\n",
    "- __\"Confidence and prediction intervals for forecasted values\"__ by __Charles Zaiontz__  available at* https://www.real-statistics.com/regression/confidence-and-prediction-intervals/ <br>\n",
    "- __\"3.7 OLS Prediction and Prediction Intervals\"__ available at* http://web.vu.lt/mif/a.buteikis/wp-content/uploads/PE_Book/3-7-UnivarPredict.html <br>\n",
    "- __\"Using python statsmodels for OLS linear regression\"__ available at* https://markthegraph.blogspot.com/2015/05/using-python-statsmodels-for-ols-linear.html <br>\n",
    "\n",
    "*Here are some great reads on these topics:* \n",
    "- __\"How to Calculate the Bias-Variance Trade-off with Python\"__ available at* https://aidevelopmenthub.com/how-to-calculate-the-bias-variance-trade-off-with-python/ <br>\n",
    "- __\"Understanding the Bias-Variance Tradeoff\"__ available at* http://scott.fortmann-roe.com/docs/BiasVariance.html <br>\n",
    "- __\"SCIKIT-LEARN : BIAS-VARIANCE TRADEOFF\"__ available at* https://www.bogotobogo.com/python/scikit-learn/scikit_machine_learning_Bias-variance-Tradeoff.php <br>\n",
    "- __\"Linear Regression Confidence Intervals\"__ available at* https://rstudio-pubs-static.s3.amazonaws.com/195401_20b3272a8bb04615ae7ee4c81d18ffb5.html <br>\n",
    "- __\"Prediction Interval: Simple Definition, Examples\"__ available at* https://www.statisticshowto.com/prediction-interval/ <br>\n",
    "\n",
    "*Here are some great videos on these topics:* \n",
    "- __\"Machine Learning Fundamentals: Bias and Variance\"__ by __StatQuest with Josh Starmer__ available at* https://www.youtube.com/watch?v=EuBBz3bI-aA <br>\n",
    "- __\"Bias Variance Trade off\"__ by __The Semicolon__ available at* https://www.youtube.com/watch?v=lpkSGTT8uMg <br>\n",
    "- __\"Intervals (for the Mean Response and a Single Response) in Simple Linear Regression\"__ by __jbstatistics__ available at* https://www.youtube.com/watch?v=V-sReSM887I <br>\n",
    "- __\"Calculate Confidence and prediction intervals for a response in SLR by hand\"__ by __Katie Ann Jager__ available at* https://www.youtube.com/watch?v=JqObYVX1UP0 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Ice cream cone baking! <br> \n",
    "\n",
    "![](https://s23991.pcdn.co/wp-content/uploads/2020/07/waffle-cones.jpg)   \n",
    "\n",
    "\n",
    "#### The 'icecreamcone.csv' file,  has the recordings on daily  temperature, relative humidity, cone strength, and cone weight based on noon readings for 20 days of cone making. Follow the steps and answer the following questions:\n",
    "\n",
    "- Step1: Read the \"icecreamcone.csv\" file as a dataframe. Explore the dataframe and in a markdown cell breifly describe it in your own words. <br>\n",
    "\n",
    "- Step2: Calculate and compare the correlation coefficient of the cone's weight with all the other parameters. In a markdown cell, explain the results and state which parameters have the strongest and weakest relationship with the cone's weight of a vehicle. \n",
    "\n",
    "- Step3: Use linear regression modeling with statsmodels, get the linear model's coefficients, make a plot and VISUALLY assess the quality of a linear fit with humidity as the predictor, and cone's weight as outcome. Then, use RMSE, Pearson's r, and NSE to describe the performance of your model. Explain the result of this analysis in a markdown cell.\n",
    "\n",
    "- Step4: Use linear regression modeling with statsmodels, get the linear model's coefficients, make a plot and VISUALLY assess the quality of a linear fit with cone's strength as the predictor, and cone's weight as outcome. Then, use RMSE, Pearson's r, and NSE to describe the performance of your model. Explain the result of this analysis in a markdown cell.\n",
    "\n",
    "- Step5: Use multiple linear regression modeling with scikit-learn and use all the three predictor parameters to predict cone's weight. Then, use RMSE, Pearson's r, and NSE to describe the performance of your model. Explain the result of this analysis in a markdown cell.\n",
    "\n",
    "- Step6: As a conclusion, make a statement about the quality of the three predictive models you wrote and compare their performances. \n",
    "\n",
    "*Data Source: V.T. Huang, S.T. Luebbers, J.B. Lindamood, P.M.T. Hansen (1989). \"Ice Cream Cone Baking: 2. Textured Characteristics of Rolled Sugar Cones,\" Food Hydrocolloids, Vol. 3, #1, pp. 41-55.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step1:Read the \"icecreamcone.csv\" file as a dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the dataframe: Describe the df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step2: Calculate and compare the correlation coefficient\n",
    "#What can we infer?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step3: humidity as the predictor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GOF metrics:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step4: strength as the predictor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GOF metrics:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step5: 3 predictor - Multiple Linear Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GOF metrics:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://img.libquotes.com/pic-quotes/v4/bruce-willis-quote-lby1n5v.jpg) <br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
